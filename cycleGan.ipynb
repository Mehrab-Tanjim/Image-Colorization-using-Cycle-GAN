{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "\n",
    "import argparse\n",
    "import itertools\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "from models import Generator\n",
    "from models import Discriminator\n",
    "from utils import ReplayBuffer\n",
    "from utils import LambdaLR\n",
    "from utils import Logger\n",
    "from utils import weights_init_normal\n",
    "from datasets import ImageDataset, Flower_Dataset\n",
    "from easydict import EasyDict\n",
    "import os\n",
    "\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import gan_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = EasyDict()\n",
    "\n",
    "opt.epoch = 0\n",
    "opt.n_epochs = 200\n",
    "opt.batchSize = 20\n",
    "opt.dataroot = 'datasets/'\n",
    "opt.lr = 0.0002\n",
    "opt.decay_epoch = 100\n",
    "opt.size = 224\n",
    "opt.input_nc = 3 #change from color to gray\n",
    "opt.output_nc = 3\n",
    "opt.n_cpu = 8\n",
    "opt.lambda_identity = 0.5\n",
    "opt.lambda_A = 10 \n",
    "opt.lambda_B = 10 #back to color is given more importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is supported\n"
     ]
    }
   ],
   "source": [
    "# Check if your system supports CUDA\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Setup GPU optimization if CUDA is supported\n",
    "if use_cuda:\n",
    "    computing_device = torch.device(\"cuda\")\n",
    "    extras = {\"num_workers\": 1, \"pin_memory\": True}\n",
    "    print(\"CUDA is supported\")\n",
    "else:  # Otherwise, train on the CPU\n",
    "    computing_device = torch.device(\"cpu\")\n",
    "    extras = False\n",
    "    print(\"CUDA NOT supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Definition of variables ######\n",
    "# Networks\n",
    "\n",
    "netG_A2B = Generator(opt.input_nc, opt.output_nc)\n",
    "netG_B2A = Generator(opt.output_nc, opt.input_nc)\n",
    "netD_A = Discriminator(opt.input_nc)\n",
    "netD_B = Discriminator(opt.output_nc)\n",
    "\n",
    "# netD_A = Discriminator(opt.input_nc)\n",
    "# netD_B = Discriminator(opt.output_nc)\n",
    "\n",
    "# netG_A2B = models.define_G(opt.input_nc, opt.output_nc, 64, 'unet_128', 'batch', 'False', 'normal', 0.02, [0])\n",
    "# netG_B2A = models.define_G(opt.input_nc, opt.output_nc, 64, 'unet_128', 'batch', 'False', 'normal', 0.02, [0])\n",
    "\n",
    "\n",
    "# netD_A = models.define_D(opt.input_nc, 64, 'basic', 3, 'batch', False, 'normal', 0.02, [0])\n",
    "# netD_B = models.define_D(opt.output_nc, 64, 'basic', 3, 'batch', False, 'normal', 0.02, [0])\n",
    "\n",
    "\n",
    "# netG_A2B = gan_model.ConvGen(opt.input_nc, opt.output_nc)\n",
    "# netG_B2A = gan_model.ConvGen(opt.output_nc, opt.input_nc)\n",
    "# netD_A = gan_model.ConvDis(opt.input_nc)\n",
    "# netD_B = gan_model.ConvDis(opt.output_nc)\n",
    "#gan_model\n",
    "\n",
    "netG_A2B = netG_A2B.to(computing_device)\n",
    "netG_B2A = netG_B2A.to(computing_device)\n",
    "netD_A = netD_A.to(computing_device)\n",
    "netD_B = netD_B.to(computing_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gan_model import *\n",
    "\n",
    "# netG_A2B = ConvGen()\n",
    "# netG_B2A = ConvGen()\n",
    "# netD_A = ConvDis()\n",
    "# netD_B = ConvDis()\n",
    "\n",
    "\n",
    "# netG_A2B = netG_A2B.to(computing_device)\n",
    "# netG_B2A = netG_B2A.to(computing_device)\n",
    "# netD_A = netD_A.to(computing_device)\n",
    "# netD_B = netD_B.to(computing_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import os  \n",
    "fileCheck = os.path.isfile('output/netG_A2B.pth') \n",
    "print(fileCheck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileCheck = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if fileCheck:\n",
    "#     print(\"previous weights are loaded\")\n",
    "#     netG_A2B.load_state_dict(torch.load('output/netG_A2B.pth'))\n",
    "#     netG_B2A.load_state_dict(torch.load('output/netG_B2A.pth'))\n",
    "#     netD_A.load_state_dict(torch.load('output/netD_A.pth'))\n",
    "#     netD_B.load_state_dict(torch.load('output/netD_B.pth'))\n",
    "    \n",
    "# else:\n",
    "#     print(\"weights are initialized\")\n",
    "#     netG_A2B.apply(weights_init_normal)\n",
    "#     netG_B2A.apply(weights_init_normal)\n",
    "#     netD_A.apply(weights_init_normal)\n",
    "#     netD_B.apply(weights_init_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lossess\n",
    "criterion_GAN = torch.nn.MSELoss() #torch.nn.BCELoss() if need to use other loss\n",
    "criterion_cycle = torch.nn.L1Loss()\n",
    "criterion_identity = torch.nn.L1Loss()\n",
    "\n",
    "# Optimizers & LR schedulers\n",
    "optimizer_G = torch.optim.Adam(itertools.chain(netG_A2B.parameters(), netG_B2A.parameters()),\n",
    "                                lr=opt.lr, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(itertools.chain(netD_A.parameters(), netD_B.parameters()),\n",
    "                                lr=opt.lr, betas=(0.5, 0.999))\n",
    "\n",
    "# lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=LambdaLR(opt.n_epochs, opt.epoch, opt.decay_epoch).step)\n",
    "# lr_scheduler_D = torch.optim.lr_scheduler.LambdaLR(optimizer_D, lr_lambda=LambdaLR(opt.n_epochs, opt.epoch, opt.decay_epoch).step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs & targets memory allocation\n",
    "Tensor = torch.cuda.FloatTensor if use_cuda else torch.Tensor\n",
    "input_A = Tensor(opt.batchSize, opt.input_nc, opt.size, opt.size)\n",
    "input_B = Tensor(opt.batchSize, opt.output_nc, opt.size, opt.size)\n",
    "\n",
    "target_real = Variable(Tensor(opt.batchSize).fill_(1.0), requires_grad=False)\n",
    "target_fake = Variable(Tensor(opt.batchSize).fill_(0.0), requires_grad=False)\n",
    "\n",
    "fake_A_buffer = ReplayBuffer()\n",
    "fake_B_buffer = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_ = [# transforms.Resize(int(opt.size*1.12), Image.BICUBIC), \n",
    "                #transforms.RandomCrop(opt.size), \n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #split train and validation data\n",
    "# import numpy as np\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# images_gray = np.load(opt.dataroot+'A/gray_scale.npy')\n",
    "# images_lab = np.load(opt.dataroot+'B/ab1.npy')\n",
    "# np.save(opt.dataroot + 'Train/A/gray_scale.npy', images_gray[:300])\n",
    "# np.save(opt.dataroot + 'Train/B/ab1.npy', images_lab[:300] )\n",
    "# images_gray = np.load(opt.dataroot + 'Train/A/gray_scale.npy')\n",
    "# images_lab = np.load(opt.dataroot + 'Train/B/ab1.npy')\n",
    "# plt.figure()\n",
    "# plt.imshow(images_gray[29],cmap='gray')\n",
    "# plt.show()\n",
    "\n",
    "# images_gray = np.load(opt.dataroot+'A/gray_scale.npy')\n",
    "# images_lab = np.load(opt.dataroot+'B/ab1.npy')\n",
    "# np.save(opt.dataroot + 'Test/A/gray_scale.npy', images_gray[1000:1030])\n",
    "# np.save(opt.dataroot + 'Test/B/ab1.npy', images_lab[1000:1030])\n",
    "# images_gray = np.load(opt.dataroot + 'Test/A/gray_scale.npy')\n",
    "# images_lab = np.load(opt.dataroot + 'Test/B/ab1.npy')\n",
    "# plt.figure()\n",
    "# plt.imshow(images_gray[0],cmap='gray')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2, decode_predictions, preprocess_input\n",
    "\n",
    "# def get_rbg_from_lab(gray_imgs, ab_imgs, n = 10):\n",
    "#     imgs = np.zeros((n, 224, 224, 3))\n",
    "#     imgs[:, :, :, 0] = gray_imgs[0:n:]\n",
    "#     imgs[:, :, :, 1:] = ab_imgs[0:n:]\n",
    "\n",
    "#     imgs = imgs.astype(\"uint8\")\n",
    "\n",
    "#     imgs_ = []\n",
    "#     for i in range(0, n):\n",
    "#         imgs_.append(cv2.cvtColor(imgs[i], cv2.COLOR_LAB2RGB))\n",
    "\n",
    "#     imgs_ = np.array(imgs_)\n",
    "\n",
    "# #         print(imgs_.shape)\n",
    "\n",
    "#     return imgs_\n",
    "\n",
    "# def pipe_line_img(gray_scale_imgs, batch_size = 100, preprocess_f = preprocess_input):\n",
    "#     imgs = np.zeros((batch_size, 224, 224, 3))\n",
    "#     for i in range(0, 3):\n",
    "#         imgs[:batch_size, :, :,i] = gray_scale_imgs[:batch_size]\n",
    "#     return preprocess_f(imgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "\n",
    "# images_gray = np.load('datasets/Train/A/gray_scale.npy')\n",
    "# files_A = pipe_line_img(images_gray, batch_size = images_gray.shape[0]).transpose(0, 3, 1, 2)\n",
    "# img = files_A[0].transpose(1, 2, 0)\n",
    "# # plt.imsave('output/test.png', [img, img])\n",
    "\n",
    "\n",
    "# img_list = []\n",
    "\n",
    "# image_list = [img, img, img]\n",
    "\n",
    "# #np.concatenate((image_list), 1)\n",
    "\n",
    "# img_list.append(np.concatenate((image_list), 1))\n",
    "# img_list = np.array(img_list)\n",
    "# img_list = np.squeeze(img_list, axis=0)\n",
    "# # print(img_list.shape)\n",
    "# plt.imsave('output/test.png', img_list)\n",
    "# plt.axis(\"off\")\n",
    "\n",
    "# # total_width = 2*224\n",
    "# # max_height = 224\n",
    "\n",
    "\n",
    "# # new_im = Image.new('RGB', (total_width, max_height))\n",
    "# # print(files_A[0].shape)\n",
    "# # x_offset = 0\n",
    "# # for im in range(2):\n",
    "# #     img = files_A[im].transpose(1, 2, 0)\n",
    "# #     print(img.shape)\n",
    "# #     img = Image.fromarray(files_A[im], 'RGE')\n",
    "# #     new_im.paste(img, (x_offset,0))\n",
    "# #     img.save('output/'+str(im)+'.png')\n",
    "# #     img.show()\n",
    "# #     x_offset += 224\n",
    "\n",
    "# # new_im.save('output/test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in here\n",
      "rearranged (300, 3, 224, 224)\n",
      "in here\n",
      "rearranged (30, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "trainDataloader = DataLoader(ImageDataset(opt.dataroot + 'Train/', transforms_ = transforms_), batch_size=opt.batchSize, shuffle=True, num_workers=opt.n_cpu)\n",
    "testDataloader = DataLoader(ImageDataset(opt.dataroot + 'Test/', transforms_ = transforms_), batch_size=opt.batchSize, shuffle=False, num_workers=opt.n_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 1020 images, used 0.049773s\n",
      "Load 340 images, used 0.013726s\n"
     ]
    }
   ],
   "source": [
    "from datasets import Flower_Dataset as myDataset\n",
    "\n",
    "data_train = myDataset( opt.dataroot,\n",
    "        shuffle=True,\n",
    "        small=False,\n",
    "        mode='train',\n",
    "        transform=transforms.Compose(transforms_),\n",
    "        target_transform=None,\n",
    "        types='')\n",
    "trainDataloader = data.DataLoader(data_train,\n",
    "                               batch_size= opt.batchSize,\n",
    "                               shuffle=True,\n",
    "                               num_workers=4)\n",
    "data_val = myDataset(opt.dataroot,\n",
    "                    shuffle=False,\n",
    "                    small=False,\n",
    "                    mode='test',\n",
    "                    transform=transforms.Compose(transforms_),\n",
    "                    target_transform=None,\n",
    "                    types=''\n",
    "                      )\n",
    "\n",
    "testDataloader = data.DataLoader(data_val,\n",
    "                                  batch_size=opt.batchSize,\n",
    "                                  shuffle=False,\n",
    "                                  num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testDataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss plot\n",
    "def validation(opt, val_logger, testDataloader, netG_A2B, netG_B2A, netD_A, netD_B, criterion_identity, criterion_GAN, criterion_cycle):\n",
    "\n",
    "    netG_A2B.eval()\n",
    "    netG_B2A.eval()\n",
    "    netD_A.eval()\n",
    "    netD_B.eval()\n",
    "    \n",
    "    lambda_idt = opt.lambda_identity\n",
    "    lambda_A = opt.lambda_A\n",
    "    lambda_B = opt.lambda_B\n",
    "\n",
    "    ###################################\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, batch in enumerate(testDataloader):\n",
    "            # Set model input\n",
    "    #         real_A = Variable(input_A.copy_(batch['A'])) #gray image\n",
    "    #         real_B = Variable(input_B.copy_(batch['B'])) #colored image\n",
    "\n",
    "#             real_B = Variable(input_A.copy_(batch['A'])) #gray image\n",
    "#             real_A = Variable(input_B.copy_(batch['B'])) #colored image\n",
    "\n",
    "            real_A = Variable(input_A.copy_(batch[0])) #gray image\n",
    "            real_B = Variable(input_B.copy_(batch[1])) #colored image\n",
    "\n",
    "            ###### Generators A2B and B2A ######\n",
    "\n",
    "            # generate fakes \n",
    "\n",
    "            fake_B = netG_A2B(real_A)\n",
    "            recovered_A = netG_B2A(fake_B)\n",
    "\n",
    "            fake_A = netG_B2A(real_B)\n",
    "            recovered_B = netG_A2B(fake_A)\n",
    "\n",
    "\n",
    "            # Identity loss, condition         \n",
    "            if lambda_idt > 0:\n",
    "                # should be identity if real_B is fed.\n",
    "                loss_identity_B = criterion_identity(netG_A2B(real_B), real_B) * lambda_B * lambda_idt\n",
    "                # should be identity if real_A is fed.\n",
    "                loss_identity_A = criterion_identity(netG_B2A(real_A), real_A) * lambda_A * lambda_idt\n",
    "            else:\n",
    "                loss_identity_B = 0\n",
    "                loss_identity_A = 0\n",
    "\n",
    "            # GAN loss\n",
    "            loss_GAN_A2B = criterion_GAN(netD_B(fake_B), target_real)\n",
    "            loss_GAN_B2A = criterion_GAN(netD_A(fake_A), target_real)\n",
    "\n",
    "\n",
    "            # Cycle loss\n",
    "            loss_cycle_ABA = criterion_cycle(recovered_A, real_A) * lambda_A\n",
    "            loss_cycle_BAB = criterion_cycle(recovered_B, real_B) * lambda_B\n",
    "\n",
    "\n",
    "            # combined loss\n",
    "            loss_G = loss_identity_A + loss_identity_B + loss_GAN_A2B + loss_GAN_B2A + loss_cycle_ABA + loss_cycle_BAB\n",
    "            \n",
    "\n",
    "            ###################################\n",
    "            ###### Discriminator A ######\n",
    "\n",
    "            # Real loss\n",
    "            loss_D_real = criterion_GAN(netD_A(real_A), target_real)\n",
    "\n",
    "            # Fake loss\n",
    "            loss_D_fake = criterion_GAN(netD_A(fake_A_buffer.push_and_pop(fake_A).detach()), target_fake)\n",
    "\n",
    "            # Total loss\n",
    "            loss_D_A = (loss_D_real + loss_D_fake)*0.5\n",
    "            \n",
    "\n",
    "            ###################################\n",
    "\n",
    "            ###### Discriminator B ######\n",
    "\n",
    "            # Real loss\n",
    "            loss_D_real = criterion_GAN(netD_B(real_B), target_real)\n",
    "\n",
    "            # Fake loss\n",
    "            loss_D_fake = criterion_GAN(netD_B(fake_B_buffer.push_and_pop(fake_B).detach()), target_fake)\n",
    "\n",
    "            # Total loss\n",
    "            loss_D_B = (loss_D_real + loss_D_fake)*0.5\n",
    "            \n",
    "\n",
    "            ###################################\n",
    "\n",
    "            # Progress report (http://localhost:8097)\n",
    "            val_logger.log({'loss_G': loss_G, 'loss_G_identity': (loss_identity_A + loss_identity_B), 'loss_G_GAN': (loss_GAN_A2B + loss_GAN_B2A),\n",
    "                        'loss_G_cycle': (loss_cycle_ABA + loss_cycle_BAB), 'loss_D': (loss_D_A + loss_D_B)}, \n",
    "                        images={'real_A': real_A, \n",
    "                            'real_B': real_B, \n",
    "                            'fake_A': fake_A, \n",
    "                            'fake_B': fake_B}, mode='validation',\n",
    "                        model={'netG_A2B': netG_A2B.state_dict(),'netG_B2A': netG_B2A.state_dict(),'netD_A': netD_A.state_dict(),'netD_B': netD_B.state_dict()}, lab=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_logger = Logger(opt.n_epochs, len(testDataloader))\n",
    "# for epoch in range(opt.epoch, opt.n_epochs):\n",
    "#     for i, batch in enumerate(testDataloader):\n",
    "#         validation(opt, val_logger, testDataloader, netG_A2B, netG_B2A, netD_A, netD_B, criterion_identity, criterion_GAN, criterion_cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_logger = Logger(opt.n_epochs, len(testDataloader))\n",
    "# for i in range(4):\n",
    "#       validation(opt, val_logger, testDataloader, netG_A2B, netG_B2A, netD_A, netD_B, criterion_identity, criterion_GAN, criterion_cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_logger = Logger(opt.n_epochs, len(testDataloader))\n",
    "\n",
    "\n",
    "# from skimage import color\n",
    "# import numpy as np\n",
    "# from matplotlib import pyplot as plt\n",
    "# import cv2\n",
    "# ###### Training ######\n",
    "# for epoch in range(opt.epoch, opt.n_epochs):\n",
    "#     for i, batch in enumerate(testDataloader):\n",
    "#             real_A = Variable(input_A.copy_(batch[0])) #gray image\n",
    "#             real_B = Variable(input_B.copy_(batch[1])) #colored image\n",
    "            \n",
    "#             img = real_B.cpu().numpy()[0]\n",
    "            \n",
    "#             plt.figure()\n",
    "#             plt.imshow(img)#real_B.cpu().numpy()[0].transpose(1,2,0)[:,:,0])\n",
    "#             plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/datasets/home/72/372/mtanjim/.local/lib/python3.6/site-packages/skimage/color/colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 18 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "/datasets/home/72/372/mtanjim/.local/lib/python3.6/site-packages/skimage/color/colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 252 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current train total loss tensor(11.1525)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/datasets/home/72/372/mtanjim/.local/lib/python3.6/site-packages/skimage/color/colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 1 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current validation total loss tensor(12.4385)\n",
      "current is the best loss\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/datasets/home/72/372/mtanjim/.local/lib/python3.6/site-packages/skimage/color/colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 19 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "/datasets/home/72/372/mtanjim/.local/lib/python3.6/site-packages/skimage/color/colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 29 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current train total loss tensor(10.9196)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/datasets/home/72/372/mtanjim/.local/lib/python3.6/site-packages/skimage/color/colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 60 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "/datasets/home/72/372/mtanjim/.local/lib/python3.6/site-packages/skimage/color/colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 472 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current validation total loss tensor(11.3455)\n",
      "current is the best loss\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/datasets/home/72/372/mtanjim/.local/lib/python3.6/site-packages/skimage/color/colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 957 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current train total loss tensor(10.8411)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/datasets/home/72/372/mtanjim/.local/lib/python3.6/site-packages/skimage/color/colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 3 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "/datasets/home/72/372/mtanjim/.local/lib/python3.6/site-packages/skimage/color/colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 1352 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current validation total loss tensor(9.2693)\n",
      "current is the best loss\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/datasets/home/72/372/mtanjim/.local/lib/python3.6/site-packages/skimage/color/colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 228 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current train total loss tensor(10.4391)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/datasets/home/72/372/mtanjim/.local/lib/python3.6/site-packages/skimage/color/colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 4 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "/datasets/home/72/372/mtanjim/.local/lib/python3.6/site-packages/skimage/color/colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 343 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current validation total loss tensor(11.0102)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/datasets/home/72/372/mtanjim/.local/lib/python3.6/site-packages/skimage/color/colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 299 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current train total loss tensor(10.5040)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/datasets/home/72/372/mtanjim/.local/lib/python3.6/site-packages/skimage/color/colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 10 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "/datasets/home/72/372/mtanjim/.local/lib/python3.6/site-packages/skimage/color/colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 691 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current validation total loss tensor(11.3542)\n",
      "\n",
      "current train total loss tensor(10.2503)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/datasets/home/72/372/mtanjim/.local/lib/python3.6/site-packages/skimage/color/colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 209 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current validation total loss tensor(9.5602)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/datasets/home/72/372/mtanjim/.local/lib/python3.6/site-packages/skimage/color/colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 132 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current train total loss tensor(9.9675)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/datasets/home/72/372/mtanjim/.local/lib/python3.6/site-packages/skimage/color/colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 513 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current validation total loss tensor(10.4551)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/datasets/home/72/372/mtanjim/.local/lib/python3.6/site-packages/skimage/color/colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 292 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current train total loss tensor(10.1700)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/datasets/home/72/372/mtanjim/.local/lib/python3.6/site-packages/skimage/color/colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 341 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current validation total loss tensor(9.1915)\n",
      "current is the best loss\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-4212b8291de8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Total loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mloss_D_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss_D_real\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_D_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mloss_D_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m###################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loss plot\n",
    "train_logger = Logger(opt.n_epochs, len(trainDataloader))\n",
    "val_logger = Logger(opt.n_epochs, len(testDataloader))\n",
    "\n",
    "        \n",
    "lambda_idt =  opt.lambda_identity\n",
    "lambda_A = opt.lambda_A\n",
    "lambda_B = opt.lambda_B\n",
    "###################################\n",
    "\n",
    "###### Training ######\n",
    "for epoch in range(opt.epoch, opt.n_epochs):\n",
    "    for i, batch in enumerate(trainDataloader):\n",
    "        # Set model input\n",
    "#         real_A = Variable(input_A.copy_(batch['A'])) #gray image\n",
    "#         real_B = Variable(input_B.copy_(batch['B'])) #colored image\n",
    "\n",
    "#         real_B = Variable(input_A.copy_(batch['A'])) #gray image\n",
    "#         real_A = Variable(input_B.copy_(batch['B'])) #colored image\n",
    "\n",
    "        real_A = Variable(input_A.copy_(batch[0])) #gray image\n",
    "        real_B = Variable(input_B.copy_(batch[1])) #colored image\n",
    "\n",
    "        ###### Generators A2B and B2A ######\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        \n",
    "        # generate fakes \n",
    "        \n",
    "        fake_B = netG_A2B(real_A)\n",
    "        recovered_A = netG_B2A(fake_B)\n",
    "        \n",
    "        fake_A = netG_B2A(real_B)\n",
    "        recovered_B = netG_A2B(fake_A)\n",
    "        \n",
    "       \n",
    "        # Identity loss, condition         \n",
    "        if lambda_idt > 0:\n",
    "            # should be identity if real_B is fed.\n",
    "            loss_identity_B = criterion_identity(netG_A2B(real_B), real_B) * lambda_B * lambda_idt\n",
    "            # should be identity if real_A is fed.\n",
    "            loss_identity_A = criterion_identity(netG_B2A(real_A), real_A) * lambda_A * lambda_idt\n",
    "        else:\n",
    "            loss_identity_B = 0\n",
    "            loss_identity_A = 0\n",
    "            \n",
    "        # GAN loss\n",
    "        loss_GAN_A2B = criterion_GAN(netD_B(fake_B), target_real)\n",
    "        loss_GAN_B2A = criterion_GAN(netD_A(fake_A), target_real)\n",
    "        \n",
    "        \n",
    "        # Cycle loss\n",
    "        loss_cycle_ABA = criterion_cycle(recovered_A, real_A) * lambda_A\n",
    "        loss_cycle_BAB = criterion_cycle(recovered_B, real_B) * lambda_B\n",
    "        \n",
    "        \n",
    "        # combined loss\n",
    "        loss_G = loss_identity_A + loss_identity_B + loss_GAN_A2B + loss_GAN_B2A + loss_cycle_ABA + loss_cycle_BAB\n",
    "        loss_G.backward()\n",
    "        \n",
    "        optimizer_G.step()\n",
    "        ###################################\n",
    "        optimizer_D.zero_grad()\n",
    "        ###### Discriminator A ######\n",
    "        \n",
    "        # Real loss\n",
    "        loss_D_real = criterion_GAN(netD_A(real_A), target_real)\n",
    "\n",
    "        # Fake loss\n",
    "        loss_D_fake = criterion_GAN(netD_A(fake_A_buffer.push_and_pop(fake_A).detach()), target_fake)\n",
    "\n",
    "        # Total loss\n",
    "        loss_D_A = (loss_D_real + loss_D_fake)*0.5\n",
    "        loss_D_A.backward()\n",
    "\n",
    "        ###################################\n",
    "\n",
    "        ###### Discriminator B ######\n",
    "\n",
    "        # Real loss\n",
    "        loss_D_real = criterion_GAN(netD_B(real_B), target_real)\n",
    "        \n",
    "        # Fake loss\n",
    "        loss_D_fake = criterion_GAN(netD_B(fake_B_buffer.push_and_pop(fake_B).detach()), target_fake)\n",
    "\n",
    "        # Total loss\n",
    "        loss_D_B = (loss_D_real + loss_D_fake)*0.5\n",
    "        loss_D_B.backward()\n",
    "\n",
    "        optimizer_D.step()\n",
    "        ###################################\n",
    "\n",
    "        # Progress report (http://localhost:8097)\n",
    "        train_logger.log({'loss_G': loss_G, 'loss_G_identity': (loss_identity_A + loss_identity_B), \n",
    "                          'loss_G_GAN': (loss_GAN_A2B + loss_GAN_B2A),\n",
    "                    'loss_G_cycle': (loss_cycle_ABA + loss_cycle_BAB), 'loss_D': (loss_D_A + loss_D_B)}, \n",
    "                    images={'real_A': real_A,\n",
    "                            'real_B': real_B, \n",
    "                            'fake_A': fake_A, \n",
    "                            'fake_B': fake_B}, mode = 'train', lab=False)\n",
    "        \n",
    "    validation(opt, val_logger, testDataloader, netG_A2B, netG_B2A, netD_A, netD_B, criterion_identity, criterion_GAN, criterion_cycle)\n",
    "\n",
    "    #Update learning rates\n",
    "#     lr_scheduler_G.step()\n",
    "#     lr_scheduler_D.step()\n",
    "\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(100):\n",
    "#       # Progress report (http://localhost:8097)\n",
    "#     logger.log({'loss_G': loss_G, 'loss_G_identity': (loss_identity_A + loss_identity_B), 'loss_G_GAN': (loss_GAN_A2B + loss_GAN_B2A),\n",
    "#                 'loss_G_cycle': (loss_cycle_ABA + loss_cycle_BAB), 'loss_D': (loss_D_A + loss_D_B)}, \n",
    "#                 images={'real_A': real_A, 'real_B': real_B, 'fake_A': fake_A, 'fake_B': fake_B})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(100):\n",
    "#       # Progress report (http://localhost:8097)\n",
    "#     logger.log({'loss_G': loss_G, 'loss_G_identity': (loss_identity_A + loss_identity_B), 'loss_G_GAN': (loss_GAN_A2B + loss_GAN_B2A),\n",
    "#                 'loss_G_cycle': (loss_cycle_ABA + loss_cycle_BAB), 'loss_D': (loss_D_A + loss_D_B)}, \n",
    "#                 images={'real_A': real_A, 'real_B': real_B, 'fake_A': fake_A, 'fake_B': fake_B})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
